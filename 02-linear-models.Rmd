# The Linear Model

## Straight line relationships are described using two parameters

Its all about $y = ax + b$ (or $y = mx + c$, depending on where you went to school). These two equivalent formulae are the standard high-school equations for describing a straight line. They represent how the quantity $y$ changes as $x$ does. 

As a refresher, $a$ tells us how much $y$ increases for every unit increase in $x$. Here's an example for the equation $y = 4x$ 

```{r, fig.height=5, fig.width=5, echo=FALSE}
plot.new()
plot.window(xlim = c(-4,4), ylim = c(-10,10) )
abline( a = 0, b = 4)
abline(h=0, v=0, lty="dashed")
axis(1)
axis(2)
box()
title(main="A graph of y = 4x", xlab="x", ylab="y")
```

If we play about with that value, the slope of the line changes, the $b$ term is known as the slope, or gradient, or more often because it is just a multiplier of $x$ its called the coefficient. Here's some different coefficients just to prove that point

```{r, fig.height=5, fig.width=5, echo=FALSE}
plot.new()
plot.window(xlim = c(-4,4), ylim = c(-10,10) )
abline( a = 0, b = 4, )
abline( a = 0, b = 2, col = "deepskyblue" )
abline( a = 0, b = 6, col = "darkorange" )
abline(h=0, v=0, lty="dashed")
axis(1)
axis(2)
box()
title(main="Graphs of y = 2x, y = 4x, y = 6x", xlab="x", ylab="y")
```

The $b$ part of the formula just tells us how much we add on to $y$ after we've calculated the coefficient effect. It has the effect of pushing the line up and down the y-axis. When we look at the value of $y$ for $x = 0$ we get the position that the graph hits the y-axis so this number is often called the intercept. Here's a set of lines to show that.

```{r, fig.height=5, fig.width=5, echo=FALSE}
plot.new()
plot.window(xlim = c(-4,4), ylim = c(-10,10) )
abline( a = 0, b = 4, )
abline( a = -2, b = 4, col = "deepskyblue" )
abline( a = 2, b = 4, col = "darkorange" )
abline(h=0, v=0, lty="dashed")
axis(1)
axis(2)
box()
title(main="Graphs of y = 4x - 2, y = 4x, y = 4x + 2", xlab="x", ylab="y")
```


That's all we need to know about the equation of the straight line. Now we need to look at how they're a useful tool when analysing experimental data.

TODO: TASK

## Linear models try to create a linear equation from data

In this section we'll discuss what a linear model is, and how it is different yet related to the equation we've introduced above.

A linear model is a simplification of the relationship between some sets of numbers (in the simple case we will introduce here, it is two sets, but it can be more). At its heart is a straight line, with the equation we discussed above and a certain set of values for $a$, the coefficient and $b$, the intercept and some other statistics that describe the strength of the relationship.

Let's walk through building one, graphically and in R.

First we need some sets of values, $x$ and $y$. Usually, these would be from an experiment, but here I'll make some toy ones.

```{r}
set.seed("456")
x <- runif(20, 5, 15)
y <- x * 2 + rnorm(20)
plot(x,y)
```

The graph shows 20 random $x$ values between 5 and 15 plotted against 20 $y$ values which are calculated as $2x$ with a little random noise added. We can see that there is definitely a relationship (not least because we engineered it that way). The objective of the linear model is to quantify and describe the relationship in some way. Here's where the linear equation comes in, if we could come up with a line that fitted through the data we could use the linear equation of that line to roughly describe - or model - our data. Skipping to the chase there is absolutely a way to get the line from the data. The methods are described in lots of statistics books so I won't repeat them, but you may be familiar with the general methods, its the 'line of best fit' according to the ordinary least squares method. The function we need in R is `lm()` and it works like this

```{r, eval=F}
lm(y ~ x)
```

Thats it! The function `lm()` does the work, it takes a fairly odd syntax, though. The `y ~ x` bit is an R formula and describes the relationship you want to examine, you can read it as `y depends on x`. The `y` and `x` we're referring to here are the two vectors of numbers we created and plotted above.

Looking at the function output we get this 

```{r, echo=FALSE}
lm(y ~ x)
```

These are the intercept ($b$) and the coefficient of $x$ ($a$) that we need to describe the line. So our data are described by the line $y = 1.955x + 0.778$. 

So this line is a model of the data, it's a model in the sense that it is something that represents our data, but isn't it. The line alone can be useful to teach us about our data, but there's more to the linear model than just the line.


## Linear models describe relationships between variables

Beyond working out the equation of the line, the linear model process aims to quantify and describe relationships between the variables in the data, in our toy example the variables are $x$ and $y$. Specifically when we say 'relationship', we mean whether a change in the value of $x$ appears to go along with some change in the value of $y$. 

In other words, we can think of relationship as being the slope. If $x$ causes some change in $y$ when we plot it then there must be a slope. We call the slope $a$ in our equation of a line and we call it the coefficient of the $x$ term in our linear model. These are all equivalent interpretations for our purposes, slope, relationship, coefficient and $a$ just mean that $

Linear models calculate statistics to help us decide whether the coefficient/slope/$a$ of the relationship we observe is important or not.

## Not all lines of best fit are equally good

Although a line of best fit can always be calculated, the line might not be worth much. Consider two sets of very similar numbers. Here's two vectors of random numbers with the same mean and their plot. 

```{r}
x1 <- runif(20, 5, 15)
y1 <- runif(20, 5, 15)

plot(x1, y1)
```

We can definitely calculate a line that fits these, 

```{r}
lm(y1 ~ x1)
```

and it would be $y = 0.1103x + 10.6495$. But if we compare the fit of those lines, like in these plots 

```{r,fig.show="hold", out.width="50%", echo=FALSE}
plot(x,y)
abline( a = 0.778, b = 1.995 )
title(main="Graph of y = 1.995x + 0.778", xlab="x", ylab="y")
plot(x1,y1)
abline( a = 10.4695, b = 0.1103 )
title(main="Graph of y = 0.1103x + 10.4695", xlab="x1", ylab="y1")
```

we can clearly see that not all lines are created equal. The first line fits the data much more closely than the second one. We can also see that the relationship between $x$ and $y$ is much weaker in the second set than in the first (the coefficient/slope/$a$ is weaker. So a sensible linear model of our data would give us not just the equation but also measures of the believability of the line. 

## Linear models contain statistics describing the goodness of the model

The same function we've already used - `lm()` - calculates certain statisitics. We can print them using the `summary()` function.

```{r}
model <- lm(y ~ x)
summary(model)
```

This output is verbose, there are four blocks.

  1. `Model Call` - just a restatment of the function we called
  2. `Residuals` - a set of measures of the distribution of the residuals, we don't worry about this yet.
  3. `Coefficients` - the terms of the equation and their statistics; so the intercept ($b$) and the coefficient of `x` ($a$) that we've already seen and the `Estimate` (computed values of those). We see also columns of statistics for each.
  4. The model level statistics summary - some statistics that apply to the whole model.

Let's start at the bottom and look at model level summary.

### Residual Standard Error

This is the measure of how well the line fits the data. Unlike the linear equation, the linear model has an extra error term, $e$ which represents the average distance from the actual measurments to the line in the y-axis. So in a linear model we have a formula that looks like this

\begin{equation}
 y = ax + b + e
\end{equation}

The $e$ term adds something onto the y value of the whole equation; the more we need to add on to the value of the $x$ from the line to get the real $y$. Logically, the bigger $e$ is the more the error in the model overall. 

If you look at the plots again with those distance drawn in you can see quite clearly the residual error for the second model is much bigger than for the first.

```{r,fig.show="hold", out.width="50%", echo=FALSE}
plot(x,y)
abline( a = 0.778, b = 1.955 )
fitted <- predict(lm(y~x))
for (i in 1:length(fitted)){
  segments(x[i], y0=fitted[i], y1=y[i])
}
plot(x1,y1)
abline( a = 10.4695, b = 0.1103 )
fitted <- predict(lm(y1~x1))
for (i in 1:length(fitted)){
  segments(x1[i], y0=fitted[i], y1=y1[i])
}

```

### $R^2$

$R^2$ is a measure of how well the model fits the data. If you're thinking correlation coefficient here, then you're in the right area. $R^2$ describes the proportion of variance in the $y$ values that can be explained by the $x$ values. The $R^2$ always falls between 0 and 1. Closer to 1 is usually better, but it is very domain and dataset dependent. With small and biological data sets, we don't always see values close to that because of the noise of the system.

The proper one to use in most cases is the `Adjusted R-squared`.

### $R^2$ versus Residual Standard Error

So what's the difference between these two - at first glance they do the same thing. The major difference is that RSE is in the units of the data and $R^2$ is in relative units, so you can use them in different situations e.g if you want to make your model work within particular tolerances or you want to compare models in different units. 

### $F$-Statistic

The $F$-Statistic is an indicator of a relationship between the $x$ and $y$ values of the model. In effect its testing how much better the relationship is in your model relative to a model in which the relationship is completely random. When the $F$-statistic is at 1, the relationship is no stronger than a random relationship.  The further above 1 this value is, the more it is likely there is a real relationship in the model. The $p$ value here is the $p$ that this size of $F$ would occur in a  random relationship with a similar dataset size. As with the other statistics, the size of $F$ is dependent on the domain and data being analysed.

## Coefficients have statistics 

Along with these model level statistics, linear modelling with `lm()` gives us a set of statistics _per coefficient_. These measure the effect that each coefficient has. 

### Estimate

These are the `Estimate`, which is the actual value of the coefficient from the model. We will see that along with Intercept we can have models with more than one other coefficient. These are given in the units of the data.

### Std. Error

A measure of the variability of the strength of the effect, so if some $x$ points give more pronounced $y$ values at similar coefficient values, you get a higher variability of the strength. Generally lower standard error of the coefficient is good.

### $t$-value

An estimate of how extreme the coefficient value is, basically how many Standard Deviations away the estimate is from the centre of a presumed normal distribution with mean 0. It is absolutely a $t$-test $t$-value, and like in a $t$-test we want it to be high. The higher $t$ is, then the more likely that the coefficient is not at 0.

#### Wait, what?

Why would we care whether the coefficient is at 0 or not? Well, because if it is 0, then it's having no effect on the model. Consider again the equation of a line

\begin{equation}
y = ax + b
\end{equation}

If we let the coefficient $a = 0$, this happens

\begin{equation}
y = 0 x + b\\
y = b
\end{equation}

The coefficient disappears, its having no effect! 

If the coefficient is not many standard deviations away from 0, its probably not having much effect on the relationship. The $t$ value tries to work out whether, given the data, the coefficient is in anyway different to 0.

In English, we are really saying that the size if the slope is not likely to be 0. That it is not likely that there is no relationship. Which is weak inference, but is _exactly_ the same sort of inference that all the other hypothesis test make and is exactly the same interpretation

__pull_out__
A lot of researchers get the impression that $t$-tests, ANOVAs and other hypothesis tests tell you whether something is signifcant with probability $p$. This is a massive misinterpretation. They do no such thing. 

In fact what a hypothesis test tells you is how often you'd see this difference between two means of some numbers if the real difference was 0. 

This is resolutely not the same as saying they are definitely different. Just that they're not likely to be the same as 0. The $p$ in $p$ value is usually taken to mean probability, but if it stands for anything it should be 'probably not 0'. 

Hypothesis testing like this has been criticised for being weak inference, and not without reason. 

__pull_out__

Of course, this will depend on the size of the standard deviation. The noisier the data or the smaller the sample size then the larger this value will need to be to be important. 

### $Pr(>|t|)$

This weird shorthand expression is just giving the probability of getting a value larger than the $t$-value. This comes from a $t$-test within the model and takes into account the dataset size and variability, you can think of it as the $p$-value of a test asking whether the coefficient is equal to 0. So if $p$ is less than 0.05 you can say that the value of the coefficient is not likely to be 0 and therefore is having an effect on the model. 

## A non-zero slope is what matters

By looking at the $p$-value of the coefficient then, we can see whether there is a significant relationship or, more accurately a non-zero slope

We can really emphasise by looking at the plots of lines we looked at earlier.

```{r,fig.show="hold", out.width="50%", echo=FALSE}
plot(x,y)
abline( a = 0.778, b = 1.995 )
title(main="Graph of y = 1.995x + 0.778", xlab="x", ylab="y")
plot(x1,y1)
abline( a = 10.4695, b = 0.1103 )
title(main="Graph of y = 0.1103x + 10.4695", xlab="x1", ylab="y1")
```

The slope of the second plot is weaker, it's much flatter - much closer to zero, in fact given the spread of the data we aren't that confident that it isn't a flat (zero) slope, so we aren't that confident that there is a significant relationship.

It is this feature that will help us in our overall goal of using the linear model in doing the work of all the other statistical tests we commonly use. If we have a good model and a good fit, then we can make really flexible use of the slope by looking at the significance of the coefficient.

## Major points

After all that inspection of the linear model, here's what you need to remember:

  1. Linear models describe relationships between sets of numbers (variables)
  2. The creation of the model generates statistics about the goodness of the model
  3. A non-zero coefficient (slope) means there is not likely to be no relationship (!)


---------------------------- 


Pretty soon, someone would've made some complaint about variability and that we should demonstrate the extent of that, something we might've done with error bars, and then someone like me would come along and say that even that isn't optimal so you'd better use dots, like this:

```{r, echo=FALSE}
library(ggplot2)
library(tidyr)
df %>% pivot_longer(cols = c(group1, group2), names_to = "group", values_to = "value" ) %>% 
  ggplot() + aes(group, value) + geom_point() + theme_minimal()
```



